{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe5cc8a6-3f9c-4ac1-b708-71e84ab33c35",
    "outputId": "38c9ed5b-d240-4e8c-aa00-739a5a8d19c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rpy2\n",
      "  Using cached rpy2-3.4.5-py3-none-any.whl\n",
      "Requirement already satisfied: cffi>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rpy2) (1.14.5)\n",
      "Requirement already satisfied: pytz in c:\\programdata\\anaconda3\\lib\\site-packages (from rpy2) (2021.1)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-4.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from rpy2) (2.11.3)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.10.0->rpy2) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->rpy2) (1.1.1)\n",
      "Requirement already satisfied: tzdata in c:\\programdata\\anaconda3\\lib\\site-packages (from tzlocal->rpy2) (2021.2.post0)\n",
      "Collecting backports.zoneinfo\n",
      "  Using cached backports.zoneinfo-0.2.1-cp38-cp38-win_amd64.whl (38 kB)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backports.zoneinfo, pytz-deprecation-shim, tzlocal, rpy2\n",
      "Successfully installed backports.zoneinfo-0.2.1 pytz-deprecation-shim-0.1.0.post0 rpy2-3.4.5 tzlocal-4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6be0f78",
    "outputId": "31e32d28-5246-4e96-e421-125d88851a45",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "Collecting dill>=0.3.4\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: dill, multiprocess\n",
      "Successfully installed dill-0.3.4 multiprocess-0.70.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "129f6785"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3783c599"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocess import Pool\n",
    "\n",
    "import scipy\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile magic_functions.py\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocess import Pool\n",
    "\n",
    "import scipy\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from scipy import stats\n",
    "### read trans probs\n",
    "p_reg = pd.read_excel('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/prob_mats.xlsx', 'reg',index_col=0)\n",
    "p_med = pd.read_excel('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/prob_mats.xlsx', 'med',index_col=0)\n",
    "def create_network(N,nr,er,asa,bs_n,m_size):\n",
    "    \n",
    "    ### Graph generation\n",
    "    ## Total organizations\n",
    "    N=N\n",
    "    ## region specific N\n",
    "    n_regions_list=[int(0.46*N),int( 0.16*N),int( 0.38*N)]\n",
    "    if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])!=N):\n",
    "        if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N)>0:\n",
    "            \n",
    "            n_regions_list[0] = n_regions_list[0]+len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N\n",
    "        else:\n",
    "            n_regions_list[0] = n_regions_list[0]-len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])+N\n",
    "\n",
    "            \n",
    "\n",
    "    g = nx.random_partition_graph(n_regions_list, p_in= 0.60, p_out=0.15, seed=123, directed=True)\n",
    "\n",
    "    edge_list_df=pd.DataFrame(list(g.edges(data=True)))\n",
    "    edge_list_df.columns=['source','target','weight']\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    #calculate n of b,bs,s\n",
    "    nr_n=[int(nr[0]*n_regions_list[0]),int(nr[1]*n_regions_list[0]),int(nr[2]*n_regions_list[0])]\n",
    "    er_n=[int(er[0]*n_regions_list[1]),int(er[1]*n_regions_list[1]),int(er[2]*n_regions_list[1])]\n",
    "    asa_n=[int(asa[0]*n_regions_list[2]),int(asa[1]*n_regions_list[2]),int(asa[2]*n_regions_list[2])]\n",
    "\n",
    "    if (np.sum(nr_n)<n_regions_list[0]):\n",
    "        nr_n[0]=nr_n[0]+(n_regions_list[0]-np.sum(nr_n))\n",
    "    if (np.sum(er_n)<n_regions_list[1]):\n",
    "        er_n[0]=er_n[0]+(n_regions_list[1]-np.sum(er_n))\n",
    "    if (np.sum(asa_n)<n_regions_list[2]):\n",
    "        asa_n[0]=asa_n[0]+(n_regions_list[2]-np.sum(asa_n))\n",
    "    ## if bs n controlled    \n",
    "    k_diff=nr_n[2]-int((nr_n[0]+nr_n[2])/((nr_n[0]/nr_n[2])+1+bs_n))\n",
    "    nr_n[2]=nr_n[2]-k_diff\n",
    "    nr_n[0]=nr_n[0]+k_diff\n",
    "\n",
    "    k_diff=er_n[2]-int((er_n[0]+er_n[2])/((er_n[0]/er_n[2])+1+bs_n))\n",
    "    er_n[2]=er_n[2]-k_diff\n",
    "    er_n[0]=er_n[0]+k_diff\n",
    "\n",
    "    k_diff=asa_n[2]-int((asa_n[0]+asa_n[2])/((asa_n[0]/asa_n[2])+1+bs_n))\n",
    "    asa_n[2]=asa_n[2]-k_diff\n",
    "    asa_n[0]=asa_n[0]+k_diff    \n",
    "\n",
    "    # choose b , s , bs\n",
    "    #nr\n",
    "    list1=range(0,n_regions_list[0])\n",
    "    random.seed(10)\n",
    "    list1_0=random.sample(list1, nr_n[0])\n",
    "    random.seed(10)\n",
    "    list1_1=random.sample(pd.DataFrame(set(list1)-set(list1_0)).iloc[:,0].tolist(),nr_n[1])\n",
    "    random.seed(10)\n",
    "    list1_2=random.sample(pd.DataFrame(set(list1)-(set(list1_1).union(set(list1_0)))).iloc[:,0].tolist(),nr_n[2])\n",
    "\n",
    "    #eur\n",
    "    list2=range(0+n_regions_list[0],n_regions_list[1]+n_regions_list[0])\n",
    "    random.seed(10)\n",
    "    list2_0=random.sample(list2, er_n[0])\n",
    "    random.seed(10)\n",
    "    list2_1=random.sample(pd.DataFrame(set(list2)-set(list2_0)).iloc[:,0].tolist(),er_n[1])\n",
    "    random.seed(10)\n",
    "    list2_2=random.sample(pd.DataFrame(set(list2)-(set(list2_1).union(set(list2_0)))).iloc[:,0].tolist(),er_n[2])\n",
    "\n",
    "    #asi\n",
    "    list3=range(0+n_regions_list[0]+n_regions_list[1],n_regions_list[2]+n_regions_list[0]+n_regions_list[1])\n",
    "    random.seed(10)\n",
    "    list3_0=random.sample(list3, asa_n[0])\n",
    "    random.seed(10)\n",
    "    list3_1=random.sample(pd.DataFrame(set(list3)-set(list3_0)).iloc[:,0].tolist(),asa_n[1])\n",
    "    random.seed(10)\n",
    "    list3_2=random.sample(pd.DataFrame(set(list3)-(set(list3_1).union(set(list3_0)))).iloc[:,0].tolist(),asa_n[2])\n",
    "\n",
    "    #\n",
    "    nodes_frame=pd.DataFrame(range(N),columns=['nodes'])\n",
    "    \n",
    "    nodes_frame['partition']=n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia']\n",
    "\n",
    "    nodes_frame['category']=\"\"\n",
    "\n",
    "    nodes_frame['category'][list1_0]=\"buyer\"\n",
    "    nodes_frame['category'][list2_0]=\"buyer\"\n",
    "    nodes_frame['category'][list3_0]=\"buyer\"\n",
    "\n",
    "    nodes_frame['category'][list1_1]=\"both\"\n",
    "    nodes_frame['category'][list2_1]=\"both\"\n",
    "    nodes_frame['category'][list3_1]=\"both\"\n",
    "\n",
    "    nodes_frame['category'][list1_2]=\"sup\"\n",
    "    nodes_frame['category'][list2_2]=\"sup\"\n",
    "    nodes_frame['category'][list3_2]=\"sup\"\n",
    "    \n",
    "    #\n",
    "    params_sn=pd.read_csv('skew_norm_params_reg_tier_mark_size.csv',index_col=0)\n",
    "    nodes_frame['ms']=\"\"\n",
    "    ########### draw a market size based on reg and tier \n",
    "    for i in nodes_frame['nodes']:\n",
    "        ps = params_sn.loc[(params_sn['tier']==nodes_frame['category'][i])&((params_sn['reg']==nodes_frame['partition'][i]))]\n",
    "        #print(ps)\n",
    "        np.random.seed(seed=123)\n",
    "        nodes_frame['ms'][i]  = stats.skewnorm(ps['ae'], ps['loce'], ps['scalee']).rvs(1)[0]\n",
    "\n",
    "    nqn1=np.quantile(nodes_frame['ms'],0.05)\n",
    "    nqn3=np.quantile(nodes_frame['ms'],0.5)\n",
    "    \n",
    "    nodes_frame['ms']=nodes_frame['ms']+ m_size*nodes_frame['ms']\n",
    "    \n",
    "    dummy=pd.DataFrame(columns=['ms'])\n",
    "    dummy['ms']=range(0,N)\n",
    "    #dummy.head()\n",
    "    #np.quantile(node_attr['ms'],0.67)\n",
    "    for i in range(0,N):\n",
    "        if nodes_frame.iloc[i,3]<=nqn1:\n",
    "            dummy['ms'][i]=\"low\"\n",
    "        elif nodes_frame.iloc[i,3]<=nqn3:  \n",
    "            dummy['ms'][i]=\"med\"\n",
    "        else:\n",
    "            dummy['ms'][i]=\"high\"\n",
    "    #change_1        \n",
    "    nodes_frame['ms2']=dummy['ms']\n",
    "\n",
    "    \n",
    "\n",
    "    buy_list=list1_0+list2_0+list3_0\n",
    "    sup_list=list1_2+list2_2+list3_2\n",
    "\n",
    "    edge_list_df_new=edge_list_df.drop([i for i, e in enumerate(list(edge_list_df['source'])) if e in set(sup_list)],axis=0)\n",
    "    new_index=range(edge_list_df_new.shape[0])\n",
    "    edge_list_df_new.index=new_index\n",
    "    #edge_list_df_new.head()\n",
    "\n",
    "    edge_list_df_new=edge_list_df_new.drop([i for i, e in enumerate(list(edge_list_df_new['target'])) if e in set(buy_list)],axis=0)\n",
    "    new_index=range(edge_list_df_new.shape[0])\n",
    "    edge_list_df_new.index=new_index\n",
    "    \n",
    "    g = nx.DiGraph( )\n",
    "    # Add edges and edge attributes\n",
    "    for i, elrow in edge_list_df_new.iterrows():\n",
    "        g.add_edge(elrow[0], elrow[1], attr_dict=elrow[2])\n",
    "    \n",
    "    return [edge_list_df_new,nodes_frame,g]\n",
    "\n",
    "####### To generate new attributes for desired state\n",
    "####### To generate new attributes for desired state\n",
    "\"\"\"def sample_lab_attr_all_init(N,m,sdvn):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes_normal.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r2 = robjects.globalenv['sampling_for_attributes_normal']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r2(N,m,sdvn)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result)\"\"\"\n",
    "def sample_lab_attr_all_init(N):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes_normal.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r2 = robjects.globalenv['sampling_for_attributes_normal']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r2(N)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result)\n",
    "\n",
    "\n",
    "####### To generate new attributes for desired state\n",
    "def sample_lab_attr_new_B(N,reg,s_av1,s_av2):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r = robjects.globalenv['sampling_for_attributes']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r(N,reg)\n",
    "    #print(df_result_r.head())\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \"\"\"if (s_av2-s_av1)<2:\n",
    "        s_av2=np.min([s_av2+2,64.28571])\n",
    "        if (s_av2-s_av1)>0:\n",
    "            s_av1=np.max([s_av1-2,0])\n",
    "        else:\n",
    "            s_av1=np.max([s_av2-2,0])\n",
    "            \n",
    "        if s_av2>100:\n",
    "            s_av2=100\"\"\"\n",
    "                        \n",
    "    sampled_data=df_result.loc[((df_result['score']>=(s_av1)) & (df_result['score']<=(s_av2)))]\n",
    "    if sampled_data.shape[0]==0:\n",
    "        s_av=np.mean([s_av1,s_av2])\n",
    "        s_th=s_av*0.05\n",
    "        sampled_data=df_result.loc[((df_result['score']>=(s_av-s_th)) | (df_result['score']<=(s_av+s_th)))]\n",
    "\n",
    "        tmp_vector=np.abs(sampled_data['score']-s_av)\n",
    "        #tmp_vector2=np.abs(df_result['score']-s_av)\n",
    "        sampled_data=sampled_data.loc[tmp_vector==np.min(tmp_vector)]\n",
    "\n",
    "        \n",
    "    #tmp_vector=np.abs(sampled_data['score']-s_av)\n",
    "    #tmp_vector2=np.abs(df_result['score']-s_av)\n",
    "    #sampled_data=sampled_data.loc[tmp_vector==np.min(tmp_vector)]\n",
    "    return(sampled_data.sample())\n",
    "\n",
    "####### To generate new attributes for desired state\n",
    "def sample_lab_attr_new(N,reg,s_av,s_th):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r = robjects.globalenv['sampling_for_attributes']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r(N,reg)\n",
    "    #print(df_result_r.head())\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    sampled_data=df_result.loc[((df_result['score']>=(s_av-s_th)) | (df_result['score']<=(s_av+s_th)))]\n",
    "    \n",
    "    tmp_vector=np.abs(sampled_data['score']-s_av)\n",
    "    #tmp_vector2=np.abs(df_result['score']-s_av)\n",
    "    sampled_data=sampled_data.loc[tmp_vector==np.min(tmp_vector)]\n",
    "    return(sampled_data)\n",
    "#df_result.sample()\n",
    "####### To generate new attributes for desired state\n",
    "def sample_lab_attr_all(N,reg):\n",
    "    \n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r = robjects.globalenv['sampling_for_attributes']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r(N,reg)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    q1=np.quantile(df_result['score'],0.25)\n",
    "    q3=np.quantile(df_result['score'],0.75)\n",
    "    \n",
    "    df_result['state']=\"\"\n",
    "    for i in range(0,df_result.shape[0]):\n",
    "        if df_result['score'][i]<=q1:\n",
    "            df_result['state'][i]= 'low' \n",
    "        elif df_result['score'][i]<=q3:\n",
    "            df_result['state'][i]='med'\n",
    "        else:\n",
    "            df_result['state'][i]='high'\n",
    "    #df_result=df_result.loc[df_result['state']==state_label]\n",
    "    #tmp_vector=np.abs(df_result['score']-s_av)\n",
    "    #sampled_data=df_result.loc[tmp_vector==np.min(tmp_vector)]\n",
    "    #.sample()\n",
    "    return(df_result)\n",
    "\n",
    "def initial_random_attr(rows,random_p):    \n",
    "    for i in range(0,rows):\n",
    "        \n",
    "        desired_state =random.choices(['high','med','low'],random_p)[0]\n",
    "        sample_df_1=sample_lab_attr_org(desired_state , 500)\n",
    "        if i==0:\n",
    "            samples_df=sample_df_1\n",
    "        else:\n",
    "            samples_df=pd.concat([samples_df,sample_df_1],axis=0)\n",
    "    return samples_df  \n",
    "\n",
    "####### To generate new attributes for desired state\n",
    "def sample_lab_attr_org(state_label,N):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r = robjects.globalenv['sampling_for_attributes']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r(N)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    q1=np.quantile(df_result['score'],0.25)\n",
    "    q3=np.quantile(df_result['score'],0.75)\n",
    "    \n",
    "    df_result['state']=\"\"\n",
    "    for i in range(0,df_result.shape[0]):\n",
    "        if df_result['score'][i]<=q1:\n",
    "            df_result['state'][i]= 'low' \n",
    "        elif df_result['score'][i]<=q3:\n",
    "            df_result['state'][i]='med'\n",
    "        else:\n",
    "            df_result['state'][i]='high'\n",
    "    df_result=df_result.loc[df_result['state']==state_label]\n",
    "    #tmp_vector=np.abs(df_result['score']-s_av)\n",
    "    #sampled_data=df_result.loc[tmp_vector==np.min(tmp_vector)]\n",
    "    return(df_result.sample())\n",
    "\n",
    "\n",
    "\n",
    "####### To generate new attributes for desired state\n",
    "def prob_cdf(cur_sc,reg):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_cdf_r = robjects.globalenv['prob_cdf']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_cdf_r(cur_sc,reg)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)\n",
    "def prob_cdf_m(cur_sc,msh):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_cdf_m = robjects.globalenv['prob_cdf_m']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_cdf_m(cur_sc,msh)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)\n",
    "def prob_pdf(cur_sc,reg):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_pdf_r = robjects.globalenv['prob_pdf']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_pdf_r(cur_sc,reg)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)\n",
    "def prob_pdf_m(cur_sc,msh):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_pdf_m = robjects.globalenv['prob_pdf_m']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_pdf_m(cur_sc,msh)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)\n",
    "\n",
    "def simulation_continous (node_attr,edge_list_df,num_sim,w1,w2,w3,w4,w5,bs1,bs2,N,r_on,m_on,p_reg,p_med,probs_mat,probs_mat2,run_iter,alpha1,alpha2,alpha3):\n",
    "    #nodes and edges\n",
    "    N=N #number of org\n",
    "    ncons=1000  #number of consumers\n",
    "    g2=nx.random_partition_graph([N,ncons],0.5, 0.25)\n",
    "    ed=pd.DataFrame(g2.edges())\n",
    "    ed1=ed[(((ed[0]<N) & (ed[1]>N-1)) | ((ed[0]>N-1) & (ed[1]<N)))]\n",
    "    ed1[1]-=N\n",
    "    ed1.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #partition = g.graph[\"partition\"]\n",
    "    #org=list(partition[0])\n",
    "    #cons=list(partition[1])\n",
    "   \n",
    "    '''\n",
    "    #for different weight proportion of organisations\n",
    "    orgprop=[0.2,0.3,0.5]\n",
    "    orgprop = [element * N for element in orgprop]\n",
    "    orgprop = [int(x) for x in orgprop]\n",
    "    orgwt=[0.25,0.5,0.7]\n",
    "    cnt1=0\n",
    "    ed1=pd.DataFrame()\n",
    "    for i in range(len(orgprop)):\n",
    "        g2=nx.random_partition_graph([orgprop[i], ncons], 0.5, orgwt[i])\n",
    "        edg=pd.DataFrame(g2.edges())\n",
    "        edg1=edg[(((edg[0]<orgprop[i]) & (edg[1]>orgprop[i]-1)) | ((edg[0]>orgprop[i]-1) & (edg[1]<orgprop[i])))]\n",
    "        edg1[1]-=orgprop[i]\n",
    "        edg1[0]+=cnt1\n",
    "        cnt1+=orgprop[i]\n",
    "        ed1=ed1.append(edg1)\n",
    "    ed1.reset_index(inplace=True, drop=True)\n",
    "    #ed1\n",
    "    '''\n",
    "    scons=np.array(pd.read_csv('Desktop/Data/final cons scores.csv')['x'])\n",
    "\n",
    "\n",
    "\n",
    "    #add no. of consumer\n",
    "    node_attr = node_attr\n",
    "    edge_list_df = edge_list_df\n",
    "    #P's    \n",
    "    blanck_data_tot=np.empty([N,32,num_sim],dtype='object')\n",
    "    #blanck_data_tot2=np.empty([N,4,num_sim],dtype='object')\n",
    "# create initial edgelist of consumers and consumers\n",
    "    for i in tqdm (range (num_sim), desc=\"Running i ...\"): #no. of simulations\n",
    "        blanck_data=np.empty([N,32],dtype='object')\n",
    "        #blanck_data2=np.empty([N,4],dtype='object')\n",
    "\n",
    "\n",
    "        # node attr to edge attr\n",
    "        df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "        df_4=pd.DataFrame(df_3)\n",
    "        df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "        #mat_data.head()\n",
    "        edge_list_2=df_4.stack().reset_index()\n",
    "        edge_list_2.columns=['source','target','weight']\n",
    "        #edge_list_2.head()\n",
    "        edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "        #edge_list_f.head()\n",
    "        edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "        edge_list_f.columns=['source','target','weight']\n",
    "        #edge_list_f.head()\n",
    "        st = [\"high\",\"low\"]\n",
    "        \n",
    "        ###########################################################\n",
    "        for j in tqdm (range(0,N), desc=\"Running j...\"): #looping through org\n",
    "            #N=np.float(N)\n",
    "            if len(list(np.where(edge_list_f.iloc[:,1]==j)[0]))>=1:\n",
    "                ####################################################################################################    \n",
    "                ########################################## MIMETIC##################################################\n",
    "                ####################################################################################################\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_tier_ind = [i for i, e in enumerate(list(node_attr['tier'])) if e in set([node_attr.iloc[j,10]])]\n",
    "                t_node_attr = node_attr.iloc[p_tier_ind,:]\n",
    "                #t_node_attr=t_node_attr.reset_index().iloc[:,1:]\n",
    "                #t_node_attr.head()\n",
    "\n",
    "\n",
    "\n",
    "                t_node_attr_score=t_node_attr['score'].copy()\n",
    "                t_node_attr_score=t_node_attr_score.reset_index().iloc[:,1:]\n",
    "                #t_node_attr_score\n",
    "\n",
    "                #t_node_attr.index[tnr]\n",
    "\n",
    "                for tnr in range(0,t_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<t_node_attr_score['score'][tnr]:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='low'\n",
    "\n",
    "                tier_p=pd.DataFrame(t_node_attr['state'].value_counts()/np.sum(t_node_attr['state'].value_counts()))\n",
    "                tier_p=tier_p.reset_index()\n",
    "                tier_p.columns=['state','t_p']\n",
    "                #tier_p\n",
    "\n",
    "                t_tier_p=pd.merge(st,tier_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_tier_p=t_tier_p.fillna(0.01)\n",
    "                tier_p=t_tier_p\n",
    "                #tier_p\n",
    "\n",
    "                #d_tier.index\n",
    "\n",
    "                #pd.DataFrame(node_attr.iloc[p_tier_ind,-2-2-1])\n",
    "                #df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j].reset_index().iloc[:,-1]\n",
    "\n",
    "                #states and distances \n",
    "                #d_tier=pd.concat([node_attr.iloc[p_tier_ind,-2-2-1],\n",
    "                #           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "                d_tier=pd.concat([t_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                #print(Ld)\n",
    "                #d_tier=d_tier.drop([j])\n",
    "                #d_tier=d_tier.reset_index()\n",
    "\n",
    "                d_tier=d_tier.fillna(1)\n",
    "\n",
    "                #and average disances per state\n",
    "                d_tier_avg=d_tier.groupby(['state']).mean(str(j))\n",
    "                #d_tier_avg\n",
    "\n",
    "\n",
    "\n",
    "                s_tier_avg=pd.DataFrame(t_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_tier_avg=pd.merge(st,s_tier_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_tier_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                mimetic_p=pd.merge(tier_p,d_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                mimetic_p=pd.merge(mimetic_p,s_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #mimetic_p\n",
    "\n",
    "                mimetic_p.columns=['state','tier_p','cur_node','score_m']\n",
    "                mimetic_p['tier_p'] = mimetic_p['tier_p']/np.sum(mimetic_p['tier_p'])\n",
    "                #mimetic_p\n",
    "\n",
    "                #round(mimetic_p['score_m'][0])\n",
    "\n",
    "                ################################################  regulatary mem\n",
    "                region_ind = [i for i, e in enumerate(list(p_reg.columns)) if e in set([node_attr.iloc[j,9]])]\n",
    "                ms_ind = [i for i, e in enumerate(list(p_med.columns)) if e in set([node_attr.iloc[j,12]])]\n",
    "\n",
    "                h_reg=prob_pdf(round(round(mimetic_p['score_m'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(mimetic_p['score_m'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(mimetic_p['score_m'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(mimetic_p['score_m'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                mimetic_p['pbreg_m']=pbreg['pbreg']\n",
    "                mimetic_p['pbm_m']=pbm['pbm']\n",
    "                #mimetic_p\n",
    "                ####################################################################################################    \n",
    "                ########################################## Local & Global / inform reg & normative #################\n",
    "                ####################################################################################################\n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                prnt_ind = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,1]==j].iloc[:,0])] \n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                prnt_ind2 = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,0]==j].iloc[:,1])] \n",
    "\n",
    "                l_node_attr = node_attr.iloc[prnt_ind,:]\n",
    "                l_node_attr_score=l_node_attr['score'].copy()\n",
    "                l_node_attr_score=l_node_attr_score.reset_index().iloc[:,1:]\n",
    "\n",
    "\n",
    "                #len(l_node_attr.iloc[:,-2-2-1])\n",
    "\n",
    "                #l_node_attr.loc[j]\n",
    "\n",
    "                for tnr in range(0,l_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l_node_attr_score['score'][tnr]:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='low'\n",
    "\n",
    "                l2_node_attr = node_attr.iloc[prnt_ind2,:]\n",
    "                l2_node_attr_score=l2_node_attr['score'].copy()\n",
    "                l2_node_attr_score=l2_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,l2_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l2_node_attr_score['score'][tnr]:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "                #Lp1\n",
    "\n",
    "                if len(prnt_ind2)>0:\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp1 = pd.DataFrame(l_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp1 = Lp1.reset_index()\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp2 = pd.DataFrame(l2_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l2_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp2 = Lp2.reset_index()\n",
    "                    Lp1=pd.merge(st,Lp1,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp2=pd.merge(st,Lp2,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp=pd.merge(Lp1,Lp2,how=\"left\",left_on=['state_x'],right_on='state_x')\n",
    "                    #print(Lp.head())\n",
    "                    Lp['state']=bs1*Lp['state_y_x']+bs2*Lp['state_y_y']\n",
    "                    Lp=Lp.iloc[:,[0,5]]\n",
    "                    Lp.columns=['index','state']\n",
    "                    #print(Lp1.head())\n",
    "                    #print(Lp2.head())\n",
    "\n",
    "\n",
    "                else:\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp = pd.DataFrame(l_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp = Lp.reset_index()\n",
    "                    #print(Lp)\n",
    "                    Lp=pd.merge(st,Lp,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp=Lp.iloc[:,[0,2]]\n",
    "                    #print(Lp)\n",
    "                    Lp.columns=['index','state']                \n",
    "\n",
    "                    #Lp.head()\n",
    "\n",
    "                if len(prnt_ind2)>0:\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld1=pd.concat([l_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                    Lad1=Ld1.groupby(['state']).mean()\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld2=pd.concat([l2_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind2,:].index),j] ],axis=1)\n",
    "                    #Lp2.head()\n",
    "                    Lad2=Ld2.groupby(['state']).mean()\n",
    "                    Lad1=pd.merge(st,Lad1,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad2=pd.merge(st,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad=pd.merge(Lad1,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    #print(Lad)\n",
    "                    Lad['state_n']=bs1*Lad[str(j)+'_x']+bs2*Lad[str(j)+'_y']\n",
    "                    Lad=Lad.iloc[:,[0,3]]\n",
    "                    Lad.columns=['state',str(j)]\n",
    "                    Lad.index=Lad['state']\n",
    "                    Lad=Lad.iloc[:,1]\n",
    "                    #print(Lad.head())\n",
    "                    s_l1_avg=pd.DataFrame(l_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l1_avg=pd.merge(st,s_l1_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l2_avg=pd.DataFrame(l2_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l2_avg=pd.merge(st,s_l2_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l_avg=pd.merge(s_l1_avg,s_l2_avg,how=\"left\",left_on=['state'],right_on='state')\n",
    "                    #print(s_l_avg)\n",
    "                    s_l_avg['score_n']=bs1*s_l_avg['score'+'_x']+bs2*s_l_avg['score'+'_y']\n",
    "                    s_l_avg=s_l_avg.iloc[:,[0,3]]\n",
    "                    s_l_avg.columns=['state','score']\n",
    "\n",
    "                else:\n",
    "                    #states and distances \n",
    "                    Ld=pd.concat([l_node_attr.iloc[:,-2-2-1],\n",
    "                               df_4.iloc[list(node_attr.iloc[prnt_ind,:].index),j] ],axis=1)\n",
    "                    #print(Ld)\n",
    "                    #and average disances per state\n",
    "                    Lad=Ld.groupby(['state']).mean()#str(j)\n",
    "                    Lad=pd.merge(st,Lad,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "\n",
    "                    Lad=Lad.reset_index()\n",
    "\n",
    "                    s_l_avg=pd.DataFrame(l_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l_avg=pd.merge(st,s_l_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l_avg=s_l_avg.reset_index()\n",
    "\n",
    "                    #Lad.head()\n",
    "                #print(Lad)\n",
    "\n",
    "                #Lad\n",
    "\n",
    "                #s_l_avg\n",
    "\n",
    "                #print(dist_local)\n",
    "                if len(prnt_ind2)>0:\n",
    "\n",
    "                    ## state local prob and avg distance\n",
    "                    dist_local=pd.merge(Lp,Lad, how='left', left_on=['index'], right_on = ['state'])\n",
    "                    dist_local.columns=['state','local_prob','cur_node_l']\n",
    "                    #dist_local\n",
    "\n",
    "                    dist_local=pd.merge(dist_local,s_l_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                else :\n",
    "                    #bs1*s_l_avg['score'+'_x']+s_l_avg*Lad['score'+'_y']\n",
    "                    dist_local=pd.merge(Lp,Lad, how='left', left_on=['index'], right_on = ['state'])\n",
    "                    dist_local=dist_local.iloc[:,[0,1,4]]\n",
    "                    dist_local.columns=['state','local_prob','cur_node_l']\n",
    "                    #dist_local\n",
    "                    #print(s_l_avg)\n",
    "\n",
    "                    dist_local=pd.merge(dist_local,s_l_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                    dist_local=dist_local.iloc[:,[0,1,2,4]]\n",
    "\n",
    "\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_local.columns=['state','local_prob','cur_node_l','score_l']\n",
    "\n",
    "                dist_local['local_prob']=dist_local['local_prob']/np.sum(dist_local['local_prob'])\n",
    "\n",
    "                #print(dist_local)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_local['score_l'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_local['score_l'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_local['score_l'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_local['score_l'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_local['pbreg_l']=pbreg['pbreg']\n",
    "                dist_local['pbm_l']=pbm['pbm']\n",
    "                #dist_local\n",
    "\n",
    "                ## global prob\n",
    "                #glb_p=pd.DataFrame(node_attr['state'].value_counts()/np.sum(node_attr['state'].value_counts()))\n",
    "                #glb_p=glb_p.reset_index()\n",
    "                #glb_p.columns=['state','g_p']\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_region_ind = [i for i, e in enumerate(list(node_attr['partition'])) if e in set([node_attr.iloc[j,9]])]\n",
    "                r_node_attr = node_attr.iloc[p_region_ind,:]\n",
    "\n",
    "                r_node_attr_score=r_node_attr['score'].copy()\n",
    "                r_node_attr_score=r_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,r_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<r_node_attr_score['score'][tnr]:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                glb_p=pd.DataFrame(r_node_attr['state'].value_counts()/np.sum(r_node_attr['state'].value_counts()))\n",
    "\n",
    "\n",
    "                glb_p=glb_p.reset_index()\n",
    "                glb_p.columns=['state','g_p']\n",
    "\n",
    "                t_glb_p=pd.merge(st,glb_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_glb_p=t_glb_p.fillna(0.01)\n",
    "                glb_p=t_glb_p\n",
    "\n",
    "                #print(glb_p)\n",
    "                #states and distances \n",
    "                gd=pd.concat([r_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_region_ind,:].index),j] ],axis=1)\n",
    "                #print(gd)\n",
    "\n",
    "                #and average disances per state\n",
    "                gad=gd.groupby(['state']).mean(str(j))\n",
    "                gad=pd.merge(st,gad,how=\"left\",left_on=['state'],right_on='state')\n",
    "                #gad.reset_index(inplace=True)\n",
    "                #print(gad)\n",
    "                s_g_avg=pd.DataFrame(r_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_g_avg=pd.merge(st,s_g_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_g_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                dist_global=pd.merge(glb_p,gad, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_global=pd.merge(dist_global,s_g_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_global.columns=['state','glob_prob','cur_node_g','score_g']\n",
    "\n",
    "                dist_global['glob_prob'] =dist_global['glob_prob']/np.sum(dist_global['glob_prob'])\n",
    "                #print(dist_global)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_global['score_g'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_global['score_g'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_global['score_g'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_global['score_g'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_global['pbreg_g']=pbreg['pbreg']\n",
    "                dist_global['pbm_g']=pbm['pbm']\n",
    "                #dist_global\n",
    "\n",
    "                #print('glb_p')\n",
    "                if (((i+1)*(j+1)) % 5000) ==0: print(dist_global)\n",
    "                ## all memetic\n",
    "                dist_local_global=pd.merge(dist_global,dist_local, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_local_global=dist_local_global.fillna(0.01)\n",
    "\n",
    "                #dist_local_global['m_p']=dist_local_global.product(axis=1)/np.sum(dist_local_global.product(axis=1))\n",
    "                #print(dist_local_global)\n",
    "                #\n",
    "                ####################################################################################################    \n",
    "                ########################################## All_ Pressures ##########################################\n",
    "                ####################################################################################################\n",
    "                #\n",
    "                # All presures\n",
    "                all_p = pd.merge(mimetic_p,dist_local_global,how='left', left_on=['state'], right_on = ['state'])\n",
    "                all_p=all_p.fillna(0.01)\n",
    "                #all_p = pd.merge(all_p,mimetic_p,how='left', left_on=['state'], right_on = ['state'])\n",
    "                #all_p \n",
    "                #= all_p.iloc[:,[0,4,5,6]]\n",
    "\n",
    "                #0.25*all_p.iloc[:,3:5].product(axis=1)\n",
    "\n",
    "                #all_p.iloc[:,3:5]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "                #all_p\n",
    "\n",
    "                #w1*all_p['tier_p'][0]*all_p['cur_node'][0]*all_p['score_m'][0]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "\n",
    "                all_p_tpd=all_p.copy()\n",
    "                #all_p_tpd\n",
    "                all_p_new=pd.DataFrame(all_p_tpd['state'])\n",
    "                #print(all_p_new)\n",
    "                all_p_new['tier_p']=(all_p_tpd['tier_p']*all_p_tpd['cur_node'])/np.sum(all_p_tpd['tier_p']*all_p_tpd['cur_node'])\n",
    "                all_p_new['glob_prob']=(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])/np.sum(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])\n",
    "                all_p_new['local_prob']=(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])/np.sum(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])\n",
    "                all_p_new['pbreg']=(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])/np.sum(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])\n",
    "                all_p_new['pbm']=(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])/np.sum(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])\n",
    "                all_p_new['score_m']=all_p_tpd['score_m']\n",
    "                all_p_new['score_l']=all_p_tpd['score_l']\n",
    "                all_p_new['score_g']=all_p_tpd['score_g']\n",
    "\n",
    "                #pd.DataFrame(all_p_new)\n",
    "                all_p=all_p_new\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][0],all_p['pbreg_l'][0],all_p['pbreg_g'][0]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][0],all_p['pbm_l'][0],all_p['pbm_g'][0]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "\n",
    "                ptotalh=np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4)/(1+np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4))\n",
    "                ptotall=1-ptotalh\n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                #all_p\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][0]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][0]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "                rpmp=(all_p['pbreg']*all_p['pbm'])/np.sum(all_p['pbreg']*all_p['pbm'])\n",
    "                if r_on==0:\n",
    "                    rpmp[0]=1\n",
    "                    rpmp[1]=1\n",
    "                    \n",
    "                #no. of consumers connected to this org in proportion\n",
    "                #in ptotalh, add w4*object\n",
    "                #same addition in ptotall\n",
    "                #j is looping through org\n",
    "                c=np.array(ed1[ed1[0]==j][1]) #d\n",
    "                sum1=0\n",
    "                for cnt in range(len(c)):           \n",
    "                    sum1+=scons[c[cnt]]  #where is scons\n",
    "                avg1=sum1/len(c)\n",
    "                ptotalh=((np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3)+w4*avg1)/(1+np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3)+w4*avg1)))*(rpmp[0]))\n",
    "                #ptotalh=np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm))/(1+np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm)))\n",
    "                #ptotalh=((np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))/(1+np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))))*(rpmp[0]))\n",
    "                #ptotalh=ptotalh/np.sum(ptotalh)\n",
    "\n",
    "                #ptotall=1-ptotalh\n",
    "                ptotall=(1/(1+np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))))*(rpmp[1])\n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                all_p['ptotal']=all_p['ptotal']/np.sum(all_p['ptotal'])\n",
    "                #all_p\n",
    "                for k in range(ncons):\n",
    "                    cons_org=(ed1[0].value_counts()[j])/ncons  #number of consumers connected to org/ncons\n",
    "                    cons_org*=100\n",
    "                    diff=scons[k]-sample_df_1.values[0][7]\n",
    "                     #number of consumers connected to the organisation\n",
    "                    p2=(np.exp(0.5*diff+0.5*cons_org))/(1+np.exp(0.5*diff+0.5*cons_org))\n",
    "                    if(p2>0.5):\n",
    "                        if((len(ed1[(ed1[0]==j) & (ed1[1]==k)]))==0):  #if edge not found\n",
    "                            ed1=ed1.append({0:j, 1:k}, ignore_index=True)  #then make the edge\n",
    "                    elif(p2==0.5):\n",
    "                        if(random.random()>0.5):\n",
    "                            if((len(ed1[(ed1[0]==j) & (ed1[1]==k)]))==0):\n",
    "                                ed1=ed1.append({0:j, 1:k}, ignore_index=True)\n",
    "                        else:\n",
    "                            if((len(ed1[(ed1[0]==j) & (ed1[1]==k)]))>0):\n",
    "                                ed1=ed1.drop(ed1[((ed1[0] == j) & ( ed1[1] == k))].index)\n",
    "                    else:\n",
    "                        if((len(ed1[(ed1[0]==j) & (ed1[1]==k)]))>0):  #if edge found\n",
    "                            ed1=ed1.drop(ed1[((ed1[0] == j) & ( ed1[1] == k))].index)  #remove the edge\n",
    "                ''''\n",
    "                ed1.drop(ed1[ed1[0]==j].index, inplace=True)  \n",
    "                for cnt in range(ncons):\n",
    "                    if(sorg[j]>scons[cnt]):\n",
    "                        ed1=ed1.append({0:j, 1:cnt}, ignore_index=True)\n",
    "                    elif(sorg[j]==scons[cnt]):\n",
    "                        if(random.random()>0.5):\n",
    "                            ed1=ed1.append({0:j, 1:cnt}, ignore_index=True)\n",
    "                            '''\n",
    "    #edsave.append(np.array(ed1)) \n",
    "\n",
    "                #print(all_p)\n",
    "\n",
    "                #0.6224593312018546\n",
    "                \"\"\"\n",
    "                d_s_ind=np.where(all_p['ptotal']==np.max(all_p['ptotal']))[0][0]\n",
    "                \"\"\"\n",
    "                if np.count_nonzero([w1,w2,w3])!=0:\n",
    "                    if all_p['ptotal'][0]>0.6224593312018546:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.6224593312018546:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "                else:\n",
    "                    if all_p['ptotal'][0]>0.5:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.5:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "\n",
    "\n",
    "                #print(d_s_ind)\n",
    "                \"\"\"                                                                                                            \n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][d_s_ind],all_p['pbreg_l'][d_s_ind],all_p['pbreg_g'][d_s_ind]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][d_s_ind],all_p['pbm_l'][d_s_ind],all_p['pbm_g'][d_s_ind]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][d_s_ind]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][d_s_ind]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "\n",
    "                \"\"\"s_av=(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]*all_p['score_m'][d_s_ind]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]*all_p['score_l'][d_s_ind]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2]*all_p['score_g'][d_s_ind])/(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2])\"\"\"\n",
    "                \"\"\"s_av=(w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind])/(w1+w2+w3)\"\"\"\n",
    "                #    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                #    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "                if w1==0:\n",
    "                    s_av1=np.min([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w2==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w3==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                elif np.count_nonzero([w1,w2,w3])==1:\n",
    "                    s_av1=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                    s_av2=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                elif np.count_nonzero([w1,w2,w3])==0:\n",
    "                    s_av1=node_attr['score'][j]\n",
    "                    s_av2=node_attr['score'][j]\n",
    "                else:\n",
    "                    \n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #s_av\n",
    "\n",
    "                #region_ind\n",
    "\n",
    "                #print(all_p)\n",
    "                probs_mat[i,j]=np.max(all_p['ptotal'])\n",
    "                if i==0:\n",
    "                    probs_mat2[i,j]=np.max(all_p['ptotal'])\n",
    "                else:\n",
    "                    probs_mat2[i+j,:]=probs_mat[i,j]\n",
    "\n",
    "                ## hihest prob label\n",
    "                #desired_state = random.choices(list(all_p['state']),list(all_p['all']))[0]\n",
    "                #desired_state = all_p['state'][d_s_ind] \n",
    "                #desired_state\n",
    "                #desired_state = list(all_p.loc[all_p['all']==np.max(all_p['all'])]['state'])[0]\n",
    "\n",
    "\n",
    "\n",
    "                ##### draw attributes with given label\n",
    "\n",
    "                \"\"\"sample_df_1=sample_lab_attr_new(np.float(N),region_ind[0],s_av,0.05*s_av)\"\"\"\n",
    "                \"\"\"if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,s_av2+0.12)\"\"\"\n",
    "                if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],(s_av1+s_av2)/2,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,(s_av1+s_av2)/2)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                ####################################################################################################    \n",
    "                ########################################## Update  attributes ######################################\n",
    "                ####################################################################################################\n",
    "                ## update node attributes \n",
    "                for k,replc in enumerate(sample_df_1.values[0]):\n",
    "                    node_attr.iloc[j,k]=replc                    \n",
    "                ## update edge attributes\n",
    "                # node attr to edge attr\n",
    "                df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "                df_4=pd.DataFrame(df_3)\n",
    "                df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "                #mat_data.head()\n",
    "                edge_list_2=df_4.stack().reset_index()\n",
    "                edge_list_2.columns=['source','target','weight']\n",
    "                #edge_list_2.head()\n",
    "                edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "                #edge_list_f.head()\n",
    "                edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "                edge_list_f.columns=['source','target','weight']\n",
    "\n",
    "\n",
    "\n",
    "                for k,replc in enumerate(node_attr.iloc[j,:].values):\n",
    "                    blanck_data[j,k]=replc \n",
    "\n",
    "                for k,replc in enumerate(all_p.iloc[0,1:].values):\n",
    "                    blanck_data[j,k+13]=replc \n",
    "                blanck_data[j,29]=j\n",
    "                blanck_data[j,30]=i\n",
    "                blanck_data[j,31]=all_p['state'][d_s_ind]\n",
    "\n",
    "\n",
    "                #blanck_data2[:,:2,i]=np.array(edge_list_f) \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                ####2\n",
    "                ####################################################################################################    \n",
    "                ########################################## MIMETIC##################################################\n",
    "                ####################################################################################################\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_tier_ind = [i for i, e in enumerate(list(node_attr['tier'])) if e in set([node_attr.iloc[j,10]])]\n",
    "                t_node_attr = node_attr.iloc[p_tier_ind,:]\n",
    "                #t_node_attr=t_node_attr.reset_index().iloc[:,1:]\n",
    "                #t_node_attr.head()\n",
    "\n",
    "\n",
    "\n",
    "                t_node_attr_score=t_node_attr['score'].copy()\n",
    "                t_node_attr_score=t_node_attr_score.reset_index().iloc[:,1:]\n",
    "                #t_node_attr_score\n",
    "\n",
    "                #t_node_attr.index[tnr]\n",
    "\n",
    "                for tnr in range(0,t_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<t_node_attr_score['score'][tnr]:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='low'\n",
    "\n",
    "                tier_p=pd.DataFrame(t_node_attr['state'].value_counts()/np.sum(t_node_attr['state'].value_counts()))\n",
    "                tier_p=tier_p.reset_index()\n",
    "                tier_p.columns=['state','t_p']\n",
    "                #tier_p\n",
    "\n",
    "                t_tier_p=pd.merge(st,tier_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_tier_p=t_tier_p.fillna(0.01)\n",
    "                tier_p=t_tier_p\n",
    "                #tier_p\n",
    "\n",
    "                #d_tier.index\n",
    "\n",
    "                #pd.DataFrame(node_attr.iloc[p_tier_ind,-2-2-1])\n",
    "                #df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j].reset_index().iloc[:,-1]\n",
    "\n",
    "                #states and distances \n",
    "                #d_tier=pd.concat([node_attr.iloc[p_tier_ind,-2-2-1],\n",
    "                #           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "                d_tier=pd.concat([t_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                #print(Ld)\n",
    "                #d_tier=d_tier.drop([j])\n",
    "                #d_tier=d_tier.reset_index()\n",
    "\n",
    "                d_tier=d_tier.fillna(1)\n",
    "\n",
    "                #and average disances per state\n",
    "                d_tier_avg=d_tier.groupby(['state']).mean(str(j))\n",
    "                #d_tier_avg\n",
    "\n",
    "\n",
    "\n",
    "                s_tier_avg=pd.DataFrame(t_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_tier_avg=pd.merge(st,s_tier_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_tier_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                mimetic_p=pd.merge(tier_p,d_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                mimetic_p=pd.merge(mimetic_p,s_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #mimetic_p\n",
    "\n",
    "                mimetic_p.columns=['state','tier_p','cur_node','score_m']\n",
    "                mimetic_p['tier_p'] = mimetic_p['tier_p']/np.sum(mimetic_p['tier_p'])\n",
    "                #mimetic_p\n",
    "\n",
    "                #round(mimetic_p['score_m'][0])\n",
    "\n",
    "                ################################################  regulatary mem\n",
    "                region_ind = [i for i, e in enumerate(list(p_reg.columns)) if e in set([node_attr.iloc[j,9]])]\n",
    "                ms_ind = [i for i, e in enumerate(list(p_med.columns)) if e in set([node_attr.iloc[j,12]])]\n",
    "\n",
    "                h_reg=prob_pdf(round(round(mimetic_p['score_m'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(mimetic_p['score_m'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(mimetic_p['score_m'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(mimetic_p['score_m'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                mimetic_p['pbreg_m']=pbreg['pbreg']\n",
    "                mimetic_p['pbm_m']=pbm['pbm']\n",
    "                #mimetic_p\n",
    "                ####################################################################################################    \n",
    "                ########################################## Local & Global / inform reg & normative #################\n",
    "                ####################################################################################################\n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                \"\"\"prnt_ind = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,1]==j].iloc[:,0])]\"\"\" \n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                prnt_ind2 = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,0]==j].iloc[:,1])] \n",
    "\n",
    "                \"\"\"l_node_attr = node_attr.iloc[prnt_ind,:]\n",
    "                l_node_attr_score=l_node_attr['score'].copy()\n",
    "                l_node_attr_score=l_node_attr_score.reset_index().iloc[:,1:]\n",
    "\n",
    "\n",
    "                #len(l_node_attr.iloc[:,-2-2-1])\n",
    "\n",
    "                #l_node_attr.loc[j]\n",
    "\n",
    "                for tnr in range(0,l_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l_node_attr_score['score'][tnr]:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='low'\"\"\"\n",
    "\n",
    "                l2_node_attr = node_attr.iloc[prnt_ind2,:]\n",
    "                l2_node_attr_score=l2_node_attr['score'].copy()\n",
    "                l2_node_attr_score=l2_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,l2_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l2_node_attr_score['score'][tnr]:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "                #Lp1\n",
    "\n",
    "                \"\"\"if len(prnt_ind2)>0:\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp1 = pd.DataFrame(l_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp1 = Lp1.reset_index()\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp2 = pd.DataFrame(l2_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l2_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp2 = Lp2.reset_index()\n",
    "                    Lp1=pd.merge(st,Lp1,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp2=pd.merge(st,Lp2,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp=pd.merge(Lp1,Lp2,how=\"left\",left_on=['state_x'],right_on='state_x')\n",
    "                    #print(Lp.head())\n",
    "                    Lp['state']=bs1*Lp['state_y_x']+bs2*Lp['state_y_y']\n",
    "                    Lp=Lp.iloc[:,[0,5]]\n",
    "                    Lp.columns=['index','state']\n",
    "                    #print(Lp1.head())\n",
    "                    #print(Lp2.head())\n",
    "\n",
    "\n",
    "                else:\"\"\"\n",
    "                #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                Lp = pd.DataFrame(l2_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l2_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                Lp = Lp.reset_index()\n",
    "                #print(Lp)\n",
    "                Lp=pd.merge(st,Lp,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                Lp=Lp.iloc[:,[0,2]]\n",
    "                #print(Lp)\n",
    "                Lp.columns=['index','state']                \n",
    "                #print(Lp)\n",
    "                \n",
    "\n",
    "                \n",
    "                    #Lp.head()\n",
    "\n",
    "                \"\"\"if len(prnt_ind2)>0:\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld1=pd.concat([l_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                    Lad1=Ld1.groupby(['state']).mean()\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld2=pd.concat([l2_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind2,:].index),j] ],axis=1)\n",
    "                    #Lp2.head()\n",
    "                    Lad2=Ld2.groupby(['state']).mean()\n",
    "                    Lad1=pd.merge(st,Lad1,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad2=pd.merge(st,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad=pd.merge(Lad1,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    #print(Lad)\n",
    "                    Lad['state_n']=bs1*Lad[str(j)+'_x']+bs2*Lad[str(j)+'_y']\n",
    "                    Lad=Lad.iloc[:,[0,3]]\n",
    "                    Lad.columns=['state',str(j)]\n",
    "                    #print(Lad.head())\n",
    "                    s_l1_avg=pd.DataFrame(l_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l1_avg=pd.merge(st,s_l1_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l2_avg=pd.DataFrame(l2_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l2_avg=pd.merge(st,s_l2_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l_avg=pd.merge(s_l1_avg,s_l2_avg,how=\"left\",left_on=['state'],right_on='state')\n",
    "                    #print(s_l_avg)\n",
    "                    s_l_avg['score_n']=bs1*s_l_avg['score'+'_x']+bs2*s_l_avg['score'+'_y']\n",
    "                    s_l_avg=s_l_avg.iloc[:,[0,3]]\n",
    "                    s_l_avg.columns=['state','score']\n",
    "\n",
    "                else:\"\"\"\n",
    "                #states and distances \n",
    "                Ld=pd.concat([l2_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind2,:].index),j] ],axis=1)\n",
    "                #print(Ld)\n",
    "                #and average disances per state\n",
    "                Lad=Ld.groupby(['state']).mean()#str(j)\n",
    "                #print(Lad)\n",
    "                Lad=pd.merge(st,Lad,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                #print(Lad)\n",
    "\n",
    "                Lad=Lad.reset_index()\n",
    "                #print(Lad)\n",
    "\n",
    "                s_l_avg=pd.DataFrame(l2_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_l_avg=pd.merge(st,s_l_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                s_l_avg=s_l_avg.reset_index()\n",
    "\n",
    "                    #Lad.head()\n",
    "                #print(Lad)\n",
    "                #Lad.index=Lad['state']\n",
    "                #Lad=Lad.iloc[:,1:]\n",
    "                #print(Lad)\n",
    "                #Lad\n",
    "\n",
    "                #s_l_avg\n",
    "\n",
    "                #bs1*s_l_avg['score'+'_x']+s_l_avg*Lad['score'+'_y']\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                \n",
    "                dist_local=pd.merge(Lp,Lad, how='left', left_on=['index'], right_on = ['state'])\n",
    "                dist_local=dist_local.iloc[:,[0,1,4]]\n",
    "                dist_local.columns=['state','local_prob','cur_node_l']\n",
    "                #dist_local\n",
    "                #print(s_l_avg)\n",
    "\n",
    "                dist_local=pd.merge(dist_local,s_l_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_local=dist_local.iloc[:,[0,1,2,4]]\n",
    "                #print(dist_local)\n",
    "\n",
    "                #dist_local=dist_local.drop(['index'])\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_local.columns=['state','local_prob','cur_node_l','score_l']\n",
    "\n",
    "                dist_local['local_prob']=dist_local['local_prob']/np.sum(dist_local['local_prob'])\n",
    "\n",
    "                #print(dist_local)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_local['score_l'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_local['score_l'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_local['score_l'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_local['score_l'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_local['pbreg_l']=pbreg['pbreg']\n",
    "                dist_local['pbm_l']=pbm['pbm']\n",
    "                #dist_local\n",
    "\n",
    "                ## global prob\n",
    "                #glb_p=pd.DataFrame(node_attr['state'].value_counts()/np.sum(node_attr['state'].value_counts()))\n",
    "                #glb_p=glb_p.reset_index()\n",
    "                #glb_p.columns=['state','g_p']\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_region_ind = [i for i, e in enumerate(list(node_attr['partition'])) if e in set([node_attr.iloc[j,9]])]\n",
    "                r_node_attr = node_attr.iloc[p_region_ind,:]\n",
    "\n",
    "                r_node_attr_score=r_node_attr['score'].copy()\n",
    "                r_node_attr_score=r_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,r_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<r_node_attr_score['score'][tnr]:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                glb_p=pd.DataFrame(r_node_attr['state'].value_counts()/np.sum(r_node_attr['state'].value_counts()))\n",
    "\n",
    "\n",
    "                glb_p=glb_p.reset_index()\n",
    "                glb_p.columns=['state','g_p']\n",
    "\n",
    "                t_glb_p=pd.merge(st,glb_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_glb_p=t_glb_p.fillna(0.01)\n",
    "                glb_p=t_glb_p\n",
    "\n",
    "                #print(glb_p)\n",
    "                #states and distances \n",
    "                gd=pd.concat([r_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_region_ind,:].index),j] ],axis=1)\n",
    "                #print(gd)\n",
    "\n",
    "                #and average disances per state\n",
    "                gad=gd.groupby(['state']).mean(str(j))\n",
    "                gad=pd.merge(st,gad,how=\"left\",left_on=['state'],right_on='state')\n",
    "                #gad.reset_index(inplace=True)\n",
    "                #print(gad)\n",
    "                s_g_avg=pd.DataFrame(r_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_g_avg=pd.merge(st,s_g_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_g_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                dist_global=pd.merge(glb_p,gad, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_global=pd.merge(dist_global,s_g_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_global.columns=['state','glob_prob','cur_node_g','score_g']\n",
    "\n",
    "                dist_global['glob_prob'] =dist_global['glob_prob']/np.sum(dist_global['glob_prob'])\n",
    "                #print(dist_global)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_global['score_g'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_global['score_g'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_global['score_g'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_global['score_g'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_global['pbreg_g']=pbreg['pbreg']\n",
    "                dist_global['pbm_g']=pbm['pbm']\n",
    "                #dist_global\n",
    "\n",
    "                #print('glb_p')\n",
    "                #if (((i+1)*(j+1)) % 5000) ==0: print(dist_global)\n",
    "                ## all memetic\n",
    "                dist_local_global=pd.merge(dist_global,dist_local, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_local_global=dist_local_global.fillna(0.01)\n",
    "\n",
    "                #dist_local_global['m_p']=dist_local_global.product(axis=1)/np.sum(dist_local_global.product(axis=1))\n",
    "                #print(dist_local_global)\n",
    "                #\n",
    "                ####################################################################################################    \n",
    "                ########################################## All_ Pressures ##########################################\n",
    "                ####################################################################################################\n",
    "                #\n",
    "                # All presures\n",
    "                all_p = pd.merge(mimetic_p,dist_local_global,how='left', left_on=['state'], right_on = ['state'])\n",
    "                all_p=all_p.fillna(0.01)\n",
    "                #all_p = pd.merge(all_p,mimetic_p,how='left', left_on=['state'], right_on = ['state'])\n",
    "                #all_p \n",
    "                #= all_p.iloc[:,[0,4,5,6]]\n",
    "\n",
    "                #0.25*all_p.iloc[:,3:5].product(axis=1)\n",
    "\n",
    "                #all_p.iloc[:,3:5]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "                #all_p\n",
    "\n",
    "                #w1*all_p['tier_p'][0]*all_p['cur_node'][0]*all_p['score_m'][0]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "                all_p_tpd=all_p.copy()\n",
    "                #all_p_tpd\n",
    "                all_p_new=pd.DataFrame(all_p_tpd['state'])\n",
    "                #print(all_p_new)\n",
    "                all_p_new['tier_p']=(all_p_tpd['tier_p']*all_p_tpd['cur_node'])/np.sum(all_p_tpd['tier_p']*all_p_tpd['cur_node'])\n",
    "                all_p_new['glob_prob']=(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])/np.sum(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])\n",
    "                all_p_new['local_prob']=(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])/np.sum(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])\n",
    "                all_p_new['pbreg']=(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])/np.sum(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])\n",
    "                all_p_new['pbm']=(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])/np.sum(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])\n",
    "                all_p_new['score_m']=all_p_tpd['score_m']\n",
    "                all_p_new['score_l']=all_p_tpd['score_l']\n",
    "                all_p_new['score_g']=all_p_tpd['score_g']\n",
    "\n",
    "                #pd.DataFrame(all_p_new)\n",
    "                all_p=all_p_new\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][0],all_p['pbreg_l'][0],all_p['pbreg_g'][0]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][0],all_p['pbm_l'][0],all_p['pbm_g'][0]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "\n",
    "                ptotalh=np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4)/(1+np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4))\n",
    "                ptotall=1-ptotalh\n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                #all_p\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][0]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][0]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "                rpmp=(all_p['pbreg']*all_p['pbm'])/np.sum(all_p['pbreg']*all_p['pbm'])\n",
    "                if r_on==0:\n",
    "                    rpmp[0]=1\n",
    "                    rpmp[1]=1\n",
    "                \n",
    "\n",
    "                #ptotalh=np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm))/(1+np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm)))\n",
    "                ptotalh=((np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))/(1+np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))))*(rpmp[0]))\n",
    "                #ptotalh=ptotalh/np.sum(ptotalh)\n",
    "\n",
    "                #ptotall=1-ptotalh\n",
    "                ptotall=(1/(1+np.exp(w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))))*(rpmp[1])\n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                all_p['ptotal']=all_p['ptotal']/np.sum(all_p['ptotal'])\n",
    "                #all_p\n",
    "\n",
    "\n",
    "                #print(all_p)\n",
    "\n",
    "                #0.6224593312018546\n",
    "                \"\"\"\n",
    "                d_s_ind=np.where(all_p['ptotal']==np.max(all_p['ptotal']))[0][0]\n",
    "                \"\"\"\n",
    "                if np.count_nonzero([w1,w2,w3])!=0:\n",
    "                    if all_p['ptotal'][0]>0.6224593312018546:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.6224593312018546:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "                else:\n",
    "                    if all_p['ptotal'][0]>0.5:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.5:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "\n",
    "\n",
    "                #print(d_s_ind)\n",
    "                \"\"\"                                                                                                            \n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][d_s_ind],all_p['pbreg_l'][d_s_ind],all_p['pbreg_g'][d_s_ind]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][d_s_ind],all_p['pbm_l'][d_s_ind],all_p['pbm_g'][d_s_ind]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][d_s_ind]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][d_s_ind]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "\n",
    "                \"\"\"s_av=(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]*all_p['score_m'][d_s_ind]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]*all_p['score_l'][d_s_ind]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2]*all_p['score_g'][d_s_ind])/(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2])\"\"\"\n",
    "                \"\"\"s_av=(w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind])/(w1+w2+w3)\"\"\"\n",
    "                #    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                #    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "                if w1==0:\n",
    "                    s_av1=np.min([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w2==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w3==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                elif np.count_nonzero([w1,w2,w3])==1:\n",
    "                    s_av1=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                    s_av2=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                elif np.count_nonzero([w1,w2,w3])==0:\n",
    "                    s_av1=node_attr['score'][j]\n",
    "                    s_av2=node_attr['score'][j]\n",
    "                else:\n",
    "                    \n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #s_av\n",
    "\n",
    "                #region_ind\n",
    "\n",
    "                #print(all_p)\n",
    "                probs_mat[i,j]=np.max(all_p['ptotal'])\n",
    "                if i==0:\n",
    "                    probs_mat2[i,j]=np.max(all_p['ptotal'])\n",
    "                else:\n",
    "                    probs_mat2[i+j,:]=probs_mat[i,j]\n",
    "\n",
    "                ## hihest prob label\n",
    "                #desired_state = random.choices(list(all_p['state']),list(all_p['all']))[0]\n",
    "                #desired_state = all_p['state'][d_s_ind] \n",
    "                #desired_state\n",
    "                #desired_state = list(all_p.loc[all_p['all']==np.max(all_p['all'])]['state'])[0]\n",
    "\n",
    "\n",
    "\n",
    "                ##### draw attributes with given label\n",
    "\n",
    "                \"\"\"sample_df_1=sample_lab_attr_new(np.float(N),region_ind[0],s_av,0.05*s_av)\"\"\"\n",
    "                \"\"\"if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,s_av2+0.12)\"\"\"\n",
    "                if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],(s_av1+s_av2)/2,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,(s_av1+s_av2)/2)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                ####################################################################################################    \n",
    "                ########################################## Update  attributes ######################################\n",
    "                ####################################################################################################\n",
    "                ## update node attributes \n",
    "                for k,replc in enumerate(sample_df_1.values[0]):\n",
    "                    node_attr.iloc[j,k]=replc                    \n",
    "                ## update edge attributes\n",
    "                # node attr to edge attr\n",
    "                df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "                df_4=pd.DataFrame(df_3)\n",
    "                df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "                #mat_data.head()\n",
    "                edge_list_2=df_4.stack().reset_index()\n",
    "                edge_list_2.columns=['source','target','weight']\n",
    "                #edge_list_2.head()\n",
    "                edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "                #edge_list_f.head()\n",
    "                edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "                edge_list_f.columns=['source','target','weight']\n",
    "\n",
    "\n",
    "\n",
    "                for k,replc in enumerate(node_attr.iloc[j,:].values):\n",
    "                    blanck_data[j,k]=replc \n",
    "\n",
    "                for k,replc in enumerate(all_p.iloc[0,1:].values):\n",
    "                    blanck_data[j,k+13]=replc \n",
    "                blanck_data[j,29]=j\n",
    "                blanck_data[j,30]=i\n",
    "                blanck_data[j,31]=all_p['state'][d_s_ind] \n",
    "\n",
    "                #blanck_data2[:,:2,i]=np.array(edge_list_f) \n",
    "\n",
    "        blanck_data_tot[:,:,i]=pd.DataFrame(blanck_data)\n",
    "\n",
    "                \n",
    "\n",
    "#if i>= 2:\n",
    "    #if i%5==0:\n",
    "        #probs_mat_pr.append(np.prod(np.log(probs_mat[i,:]),axis=1))\n",
    "        \n",
    "        edge_list_f.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+str(i)+\"_edge_attr.csv\")\n",
    "    reshaped_bd = np.vstack(blanck_data_tot[:,:,i] for i in range(num_sim))\n",
    "    reshaped_bd_df=pd.DataFrame(reshaped_bd)\n",
    "    reshaped_bd_df.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+ \"other_node_attr.csv\")\n",
    "\n",
    "            \n",
    "    print('Complete')\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "#scenarios\n",
    "scn_params=pd.read_csv('test_scenarios_norm_sense_u_l.csv')\n",
    "\n",
    "scn_params.shape\n",
    "\n",
    "folder_location='simulation_log/testscenarios_norm_parallel_1000/'\n",
    "#folder_location='simulation_log/'\n",
    "\n",
    "def process_func(run_iter):\n",
    "    #print('@@@@@ run iter @@@@@ --' + str(run_iter))\n",
    "    nr=[0.22,0.35,0.43]\n",
    "    er=[0.38,0.13,0.50]\n",
    "    asa=[0.22,0.06,0.72]\n",
    "   \n",
    "    N=scn_params.iloc[run_iter,0]\n",
    "    bs_n=scn_params.iloc[run_iter,3]\n",
    "    m_size=scn_params.iloc[run_iter,4]\n",
    "    bs1=scn_params.iloc[run_iter,1]\n",
    "    bs2=scn_params.iloc[run_iter,2]\n",
    "    \n",
    "    ################################################### create network\n",
    "    network_created=create_network(N,nr,er,asa,bs_n,m_size)\n",
    "    #graph\n",
    "    g=network_created[2]\n",
    "    #centrality\n",
    "    deg_cent = nx.degree_centrality(g)\n",
    "    in_deg_cent = nx.in_degree_centrality(g)\n",
    "    out_deg_cent = nx.out_degree_centrality(g)\n",
    "    eigen_cent = nx.eigenvector_centrality(g)\n",
    "    #katz_cent = nx.katz_centrality(g)\n",
    "    closeness_cent = nx.closeness_centrality(g)\n",
    "    #betw_cent = nx.betweenness_centrality(g)\n",
    "    #vote_cent = nx.voterank(g)\n",
    "    deg=pd.DataFrame(list(deg_cent.values()),columns=['deg'])\n",
    "    indeg=pd.DataFrame(list(in_deg_cent.values()),columns=['indeg'])\n",
    "    outdeg=pd.DataFrame(list(out_deg_cent.values()),columns=['outdeg'])\n",
    "    eigencent=pd.DataFrame(list(eigen_cent.values()),columns=['eigdeg'])\n",
    "    closeness=pd.DataFrame(list(closeness_cent.values()),columns=['closedeg'])\n",
    "    all_net_p=pd.concat([deg,indeg,outdeg,closeness,eigencent],axis=1)\n",
    "    #tier and ms\n",
    "    nodes_frame=network_created[1]\n",
    "    #edge list\n",
    "    edge_list_df_new=network_created[0]\n",
    "    edge_list_df=edge_list_df_new.copy()\n",
    "\n",
    "\n",
    "    n_regions_list=[int(0.46*N),int( 0.16*N),int( 0.38*N)]\n",
    "    if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])!=N):\n",
    "        if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N)>0:\n",
    "\n",
    "            n_regions_list[0] = n_regions_list[0]+len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N\n",
    "        else:\n",
    "            n_regions_list[0] = n_regions_list[0]-len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])+N\n",
    "\n",
    "    #print(n_regions_list)\n",
    "    #till partition\n",
    "    ###################################################initial attributes at random one at a time\n",
    "    #### method 1\n",
    "    \n",
    "\n",
    "    \"\"\"init_samples = sample_lab_attr_all_init(np.float(N),30,10)\"\"\"\n",
    "    init_samples = sample_lab_attr_all_init(np.float(N))\n",
    "    #### method 2\n",
    "    \"\"\"init_samples1 = initial_random_attr(n_regions_list[0],np.array([0.2,0.3,0.5]))\n",
    "    init_samples1=init_samples1.reset_index().iloc[:,1:]\n",
    "    init_samples2 = initial_random_attr(n_regions_list[1],np.array([0.3,0.3,0.4]))\n",
    "    init_samples2=init_samples2.reset_index().iloc[:,1:]\n",
    "    init_samples3 = initial_random_attr(n_regions_list[2],np.array([0.5,0.3,0.2]))\n",
    "    init_samples3=init_samples3.reset_index().iloc[:,1:]\n",
    "\n",
    "    init_samples=pd.concat([init_samples1,init_samples2,init_samples3],axis=0)\"\"\"\n",
    "    \n",
    "    ### method 3\n",
    "    \"\"\"init_samples1 = sample_lab_attr_all(n_regions_list[0],1)\n",
    "    init_samples1=init_samples1.reset_index().iloc[:,1:]\n",
    "    init_samples2 = sample_lab_attr_all(n_regions_list[1],2)\n",
    "    init_samples2=init_samples2.reset_index().iloc[:,1:]\n",
    "    init_samples3 = sample_lab_attr_all(n_regions_list[2],3)\n",
    "    init_samples3=init_samples3.reset_index().iloc[:,1:]\n",
    "\n",
    "    init_samples=pd.concat([init_samples1,init_samples2,init_samples3],axis=0)\"\"\"\n",
    "    ############\n",
    "    \n",
    "    #init_samples.head()\n",
    "    init_samples=init_samples.reset_index()\n",
    "    #init_samples.head()\n",
    "    init_samples=init_samples.iloc[:,1:]\n",
    "    #init_samples.index\n",
    "    node_attr=init_samples\n",
    "    node_attr['state']=\"high\"\n",
    "\n",
    "    node_attr['partition']=\"\"\n",
    "    for i in range(0,node_attr.shape[0]):\n",
    "        if i<n_regions_list[0]:\n",
    "            node_attr['partition'][i]='NrA'\n",
    "        elif i< (n_regions_list[0]+n_regions_list[1]):\n",
    "            node_attr['partition'][i]='Eur'\n",
    "        else:\n",
    "            node_attr['partition'][i]='Asia'\n",
    "\n",
    "    #tier and MS merge with attributes\n",
    "    node_attr = pd.concat([node_attr,nodes_frame.iloc[:,2:]],axis=1)\n",
    "    #node_attr.columns\n",
    "\n",
    "    node_attr.columns=['X1..Commitment...Governance', 'X2..Traceability.and.Risk.Assessment',\n",
    "           'X3..Purchasing.Practices', 'X4..Recruitment', 'X5..Worker.Voice',\n",
    "           'X6..Monitoring', 'X7..Remedy', 'score', 'state', 'partition','tier','ms','ms2']\n",
    "    #\n",
    "\n",
    "    #node_attr.info()\n",
    "\n",
    "\n",
    "\n",
    "    # region wise reg assumption and market size assumption\n",
    "    #p_reg_org=p_reg.copy()\n",
    "    #p_med_org=p_med.copy()\n",
    "\n",
    "    #\n",
    "    init_node_attrs_df=node_attr.copy()\n",
    "    init_edge_attrs_df=edge_list_df.copy()\n",
    "\n",
    "    import os\n",
    "    os.mkdir(folder_location+\"sc_\"+str(run_iter+1))\n",
    "\n",
    "\n",
    "    node_attr.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+str(0)+ \"_node_attr.csv\")\n",
    "\n",
    "    df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "    df_4=pd.DataFrame(df_3)\n",
    "    df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "    #mat_data.head()\n",
    "    edge_list_2=df_4.stack().reset_index()\n",
    "    edge_list_2.columns=['source','target','weight']\n",
    "    #edge_list_2.head()\n",
    "    edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "    #edge_list_f.head()\n",
    "    edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "    edge_list_f.columns=['source','target','weight']\n",
    "\n",
    "    edge_list_f.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+\"initial_edge_attr.csv\")\n",
    "\n",
    "    ##### run simulation for all\n",
    "\n",
    "    print(\"@@@@@@@@@@@@@@@ -- \"+str(run_iter))\n",
    "    w1=scn_params.iloc[run_iter,5]\n",
    "    w2=scn_params.iloc[run_iter,6]\n",
    "    w3=scn_params.iloc[run_iter,7]\n",
    "    w4=scn_params.iloc[run_iter,8]  \n",
    "    w5=scn_params.iloc[run_iter,9]  \n",
    "    \n",
    "    r_on=scn_params.iloc[run_iter,10]\n",
    "    m_on=scn_params.iloc[run_iter,11]\n",
    "    alpha1=scn_params.iloc[run_iter,12]\n",
    "    alpha2=scn_params.iloc[run_iter,13]\n",
    "    alpha3=scn_params.iloc[run_iter,14]\n",
    "    \n",
    "    if N==500:\n",
    "        num_sim=20\n",
    "    else:\n",
    "        num_sim=20\n",
    "    probs_mat=np.zeros((num_sim,N))\n",
    "\n",
    "    probs_mat2=np.zeros((((num_sim-1)*N)+1,N))\n",
    "\n",
    "    ## Initial node and edge attributes\n",
    "\n",
    "    node_attr=init_node_attrs_df.copy()\n",
    "    edge_list_df=init_edge_attrs_df.copy()\n",
    "    ################################################## simulation\n",
    "    simulation_continous(node_attr=node_attr,edge_list_df=edge_list_df,num_sim=num_sim,w1=w1,w2=w2,w3=w3,w4=w4,w5=w5,bs1=bs1,bs2=bs2,N=N,r_on=r_on,m_on=m_on,p_reg=p_reg,p_med=p_med,probs_mat=probs_mat,probs_mat2=probs_mat2,run_iter=run_iter,alpha1=alpha1,alpha2=alpha2,alpha3=alpha3)\n",
    "\n",
    "    lik_probs_mat=pd.DataFrame(probs_mat)\n",
    "    lik_probs_mat2=pd.DataFrame(probs_mat2)\n",
    "    \n",
    "    lik_probs_mat.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+\"lik_probs_mat.csv\")\n",
    "    lik_probs_mat2.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+\"lik_probs_mat2.csv\")\n",
    "    del lik_probs_mat\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6473481c",
    "outputId": "38dfec52-1c06-4a95-8808-73e7bedc2066"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.05878290395809"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(50,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb816839"
   },
   "outputs": [],
   "source": [
    "# sample_lab_attr_all_init(np.float(10),30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6682bfb2"
   },
   "outputs": [],
   "source": [
    "from magic_functions import process_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf643153"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7e4dae7"
   },
   "outputs": [],
   "source": [
    "frames_list = range(0,180)#range(0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2238d4f9-7d78-4403-a5f8-a50ca52037ac"
   },
   "outputs": [],
   "source": [
    "max_pool=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "0d5e49f2fd254d38a3d54560d3426ef8"
     ]
    },
    "id": "6d8f1b12",
    "outputId": "3ca528e9-8c0f-4c6c-f4c8-d1b73aef6a42",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5e49f2fd254d38a3d54560d3426ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Pool(max_pool) as p:\n",
    "    pool_outputs = list(\n",
    "        tqdm(\n",
    "            p.imap(process_func,\n",
    "                   frames_list),\n",
    "            total=len(frames_list)\n",
    "        )\n",
    "    )    \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "simulation_all_scenarios_parallel_v_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
